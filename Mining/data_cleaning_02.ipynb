{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bf9227c",
   "metadata": {},
   "source": [
    "### Lance an iterative 5-core\n",
    "\n",
    "We apply an iterative 5-core on the user–item graph (users and books as nodes, ratings as edges). In each iteration, we first removed users with fewer than 5 ratings, then removed books with fewer than 5 ratings and items with extreme average scores (very high or very low ratings based on at most 8 interactions). This “peeling” process was repeated up to 5 times, but in practice the number of rows quickly stabilized, so we stopped once the dataset size no longer changed. The resulting ratings_core matrix is a denser and more reliable subgraph, where both users and books have sufficient interactions to support meaningful collaborative filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1391fe03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20f196b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- isbn: string (nullable = true)\n",
      " |-- rating: float (nullable = true)\n",
      " |-- book_title: string (nullable = true)\n",
      " |-- book_author: string (nullable = true)\n",
      " |-- year_of_publication: string (nullable = true)\n",
      " |-- publisher: string (nullable = true)\n",
      " |-- Summary: string (nullable = true)\n",
      " |-- Language: string (nullable = true)\n",
      " |-- Category: string (nullable = true)\n",
      "\n",
      "rows: 65056\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark = SparkSession.builder.appName(\"kcore-stage\").getOrCreate()\n",
    "\n",
    "ratings_merged_for_core = spark.read.parquet(\"export_core/ratings_merged_for_core.parquet\")\n",
    "ratings_merged_for_core.printSchema()\n",
    "print(\"rows:\", ratings_merged_for_core.count())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caa1ff92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======= Iteration 0 =======\n",
      "Rows at start: 65056\n",
      "Rows after full iteration: 64455\n",
      "\n",
      "======= Iteration 1 =======\n",
      "Rows at start: 64455\n",
      "Rows after full iteration: 64427\n",
      "\n",
      "======= Iteration 2 =======\n",
      "Rows at start: 64427\n",
      "Rows after full iteration: 64419\n",
      "\n",
      "======= Iteration 3 =======\n",
      "Rows at start: 64419\n",
      "Rows after full iteration: 64419\n",
      "Converged, stop.\n"
     ]
    }
   ],
   "source": [
    "df = ratings_merged_for_core.select(\"user_id\", \"isbn\", \"rating\")\n",
    "\n",
    "max_iter = 5\n",
    "\n",
    "for i in range(max_iter):\n",
    "    print(f\"\\n======= Iteration {i} =======\")\n",
    "    rows_before = df.count()\n",
    "    print(\"Rows at start:\", rows_before)\n",
    "\n",
    "    user_stats = df.groupBy(\"user_id\").agg(\n",
    "        F.count(\"*\").alias(\"num_ratings\")\n",
    "    )\n",
    "    users_keep = user_stats.filter(\n",
    "        F.col(\"num_ratings\") >= 5\n",
    "    ).select(\"user_id\")\n",
    "\n",
    "    df_users = df.join(users_keep, on=\"user_id\", how=\"inner\")\n",
    "\n",
    "    book_stats = df_users.groupBy(\"isbn\").agg(\n",
    "        F.count(\"*\").alias(\"num_ratings\"),\n",
    "        F.avg(\"rating\").alias(\"avg_rating\")\n",
    "    )\n",
    "\n",
    "    suspicious_books = (\n",
    "        (F.col(\"num_ratings\") <= 8) &\n",
    "        ((F.col(\"avg_rating\") >= 9.5) | (F.col(\"avg_rating\") <= 1.0))\n",
    "    )\n",
    "\n",
    "    books_keep = book_stats.filter(\n",
    "        (F.col(\"num_ratings\") >= 5) &   \n",
    "        ~suspicious_books               \n",
    "    ).select(\"isbn\")\n",
    "\n",
    "    df_new = df_users.join(books_keep, on=\"isbn\", how=\"inner\")\n",
    "\n",
    "    rows_after = df_new.count()\n",
    "    print(\"Rows after full iteration:\", rows_after)\n",
    "\n",
    "    if rows_after == rows_before:\n",
    "        print(\"Converged, stop.\")\n",
    "        df = df_new\n",
    "        break\n",
    "\n",
    "    df = df_new\n",
    "\n",
    "ratings_core = df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920d3725",
   "metadata": {},
   "source": [
    "### The second \"overlap\"\n",
    "\n",
    "Although we already removed isolated users once before the k-core step, the topology of the user–item graph changes during k-core pruning: some books and users are dropped, and a few users may lose all of their previously shared items and become isolated again. Therefore, after obtaining ratings_core we enforce the “at least one overlapping book” condition a second time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03a5af6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final rows after enforcing overlap: 64419\n",
      "Final distinct users: 3776\n",
      "Final distinct books: 1148\n"
     ]
    }
   ],
   "source": [
    "book_user_counts = ratings_core.groupBy(\"isbn\") \\\n",
    "    .agg(F.countDistinct(\"user_id\").alias(\"num_users_for_book\"))\n",
    "\n",
    "overlap_books = book_user_counts.filter(\n",
    "    F.col(\"num_users_for_book\") >= 2\n",
    ").select(\"isbn\")\n",
    "\n",
    "ratings_on_overlap_books = ratings_core.join(\n",
    "    overlap_books,\n",
    "    on=\"isbn\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "overlap_users = ratings_on_overlap_books.select(\"user_id\").distinct()\n",
    "\n",
    "ratings_final = ratings_core.join(\n",
    "    overlap_users,\n",
    "    on=\"user_id\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "print(\"Final rows after enforcing overlap:\", ratings_final.count())\n",
    "print(\"Final distinct users:\", ratings_final.select(\"user_id\").distinct().count())\n",
    "print(\"Final distinct books:\", ratings_final.select(\"isbn\").distinct().count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac2d8c4",
   "metadata": {},
   "source": [
    "9. Split the data into a ratings table and a book metadata table\n",
    "\n",
    "We keep two separate core tables: `ratings_clean`, which stores user–book interactions (`user_id`, `isbn`, `rating`), and\n",
    "`books_clean`, which stores book-level metadata.\n",
    "This separation avoids duplicating metadata for every rating, makes the rating matrix cleaner for collaborative filtering, and allows us to update book information independently from the ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eff4e928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final ratings_clean rows: 64419\n",
      "Final books_clean rows (distinct books): 1148\n"
     ]
    }
   ],
   "source": [
    "ratings_clean = ratings_final.select(\n",
    "    \"user_id\",\n",
    "    \"isbn\",\n",
    "    \"rating\"\n",
    ")\n",
    "\n",
    "print(\"Final ratings_clean rows:\", ratings_clean.count())\n",
    "\n",
    "isbn_in_ratings = ratings_clean.select(\"isbn\").distinct()\n",
    "\n",
    "books_to_clean = ratings_merged_for_core.join(\n",
    "    isbn_in_ratings,\n",
    "    on=\"isbn\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "books_before_cleaning = books_to_clean.select(\n",
    "    \"isbn\",\n",
    "    \"book_title\",\n",
    "    \"book_author\",\n",
    "    \"year_of_publication\",\n",
    "    \"publisher\",\n",
    "    \"Summary\",\n",
    "    \"Language\",\n",
    "    \"Category\"\n",
    ").dropDuplicates([\"isbn\"])\n",
    "\n",
    "print(\"Final books_clean rows (distinct books):\", books_before_cleaning.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31172cb1",
   "metadata": {},
   "source": [
    "9. Clean the book metadate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2d77311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final books_clean rows: 1148\n",
      "+---------+-------------------------------------------------------------+----------------+-------------------+------------------------+-------+--------+--------+\n",
      "|isbn     |book_title                                                   |book_author     |year_of_publication|publisher               |Summary|Language|Category|\n",
      "+---------+-------------------------------------------------------------+----------------+-------------------+------------------------+-------+--------+--------+\n",
      "|000649840|Angelas Ashes                                                |Frank Mccourt   |1994               |Harpercollins Uk        |9      |9       |9       |\n",
      "|006000438|The Death of Vishnu: A Novel                                 |Manil Suri      |2002               |Perennial               |9      |9       |9       |\n",
      "|006017143|The Night Listener                                           |Armistead Maupin|2000               |HarperCollins Publishers|9      |9       |9       |\n",
      "|006019491|Daughter of Fortune : A Novel (Oprah's Book Club (Hardcover))|Isabel Allende  |1999               |HarperCollins           |9      |9       |9       |\n",
      "|006092988|A Tree Grows in Brooklyn                                     |Betty Smith     |1998               |Perennial               |9      |9       |9       |\n",
      "+---------+-------------------------------------------------------------+----------------+-------------------+------------------------+-------+--------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def is_missing_or_unknown(col):\n",
    "    return (\n",
    "        F.col(col).isNull() |\n",
    "        (F.trim(F.col(col)) == \"\") |\n",
    "        (F.lower(F.trim(F.col(col))).isin(\"unknown\", \"n/a\", \"na\", \"null\"))\n",
    "    )\n",
    "\n",
    "# Clean book_title and book_author\n",
    "books_clean = books_before_cleaning \\\n",
    "    .withColumn(\n",
    "        \"book_title\",\n",
    "        F.when(is_missing_or_unknown(\"book_title\"),\n",
    "               F.lit(\"Unknown Title\"))\n",
    "         .otherwise(F.trim(F.col(\"book_title\")))\n",
    "    ) \\\n",
    "    .withColumn(\n",
    "        \"book_author\",\n",
    "        F.when(is_missing_or_unknown(\"book_author\"),\n",
    "               F.lit(\"Unknown Author\"))\n",
    "         .otherwise(F.trim(F.col(\"book_author\")))\n",
    "    )\n",
    "\n",
    "# Clean year_of_publication\n",
    "year_str = F.trim(F.col(\"year_of_publication\").cast(\"string\"))\n",
    "\n",
    "books_clean = books_clean.withColumn(\n",
    "    \"year_of_publication\",\n",
    "    F.when(\n",
    "        year_str.rlike(r\"^[0-9]{4}$\"),         \n",
    "        year_str.cast(\"int\")\n",
    "    ).otherwise(F.lit(None).cast(\"int\"))       \n",
    ")\n",
    "\n",
    "# Clean publisher\n",
    "books_clean = books_clean.withColumn(\n",
    "    \"publisher\",\n",
    "    F.when(is_missing_or_unknown(\"publisher\"),\n",
    "           F.lit(\"Unknown Publisher\"))\n",
    "     .otherwise(F.trim(F.col(\"publisher\")))\n",
    ")\n",
    "\n",
    "# Clean Summary\n",
    "books_clean = books_clean.withColumn(\n",
    "    \"Summary\",\n",
    "    F.when(is_missing_or_unknown(\"Summary\"),\n",
    "           F.lit(\"No Summary\"))\n",
    "     .otherwise(F.trim(F.col(\"Summary\")))\n",
    ")\n",
    "\n",
    "# Clean Language\n",
    "books_clean = books_clean.withColumn(\n",
    "    \"Language\",\n",
    "    F.when(is_missing_or_unknown(\"Language\"),\n",
    "           F.lit(\"Unknown Language\"))\n",
    "     .otherwise(F.lower(F.trim(F.col(\"Language\"))))\n",
    ")\n",
    "\n",
    "# Clean Category\n",
    "books_clean = books_clean.withColumn(\n",
    "    \"Category\",\n",
    "    F.when(is_missing_or_unknown(\"Category\"),\n",
    "           F.lit(\"Unknown Category\"))\n",
    "     .otherwise(F.trim(F.col(\"Category\")))\n",
    ")\n",
    "\n",
    "print(\"Final books_clean rows:\", books_clean.count())\n",
    "books_clean.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8daa32",
   "metadata": {},
   "source": [
    "### Exporting the cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fd3827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving ratings to: D:\\projet_esilv\\Mining\\export_core\\pdf_core_ratings.parquet\n",
      "Saving books to: D:\\projet_esilv\\Mining\\export_core\\pdf_core_books.parquet\n"
     ]
    }
   ],
   "source": [
    "pdf_core_ratings = ratings_clean.toPandas()\n",
    "pdf_core_books = books_clean.toPandas()\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "if getattr(sys, 'frozen', False):  \n",
    "    base_dir = os.path.dirname(sys.executable)\n",
    "else:  \n",
    "    base_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "out_dir = os.path.join(base_dir, \"export_core\")\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "output_path_ratings = os.path.join(out_dir, \"pdf_core_ratings.parquet\")\n",
    "output_path_books = os.path.join(out_dir, \"pdf_core_books.parquet\")\n",
    "\n",
    "print(\"Saving ratings to:\", output_path_ratings)\n",
    "print(\"Saving books to:\", output_path_books)\n",
    "\n",
    "pdf_core_ratings.to_parquet(output_path_ratings, index=False)\n",
    "pdf_core_books.to_parquet(output_path_books, index=False)\n",
    "\n",
    "\n",
    "# print(\"Saving ratings_clean (Spark) to: D:/projet_esilv/Mining/export_core/ratings_clean_spark\")\n",
    "# print(\"Saving books_clean (Spark) to: D:/projet_esilv/Mining/export_core/books_clean_spark\")\n",
    "# \n",
    "# ratings_clean.write.mode(\"overwrite\").parquet(\"export_core/ratings_clean_spark\")\n",
    "# books_clean.write.mode(\"overwrite\").parquet(\"export_core/books_clean_spark\")\n",
    "\n",
    "# base_dir = r\"D:/projet_esilv/Mining/export_core\"\n",
    "# \n",
    "# spark_path_ratings = f\"file:///{base_dir}/ratings_clean_spark\"\n",
    "# spark_path_books   = f\"file:///{base_dir}/books_clean_spark\"\n",
    "# \n",
    "# print(\"Saving ratings_clean to:\", spark_path_ratings)\n",
    "# \n",
    "# ratings_clean.write.mode(\"overwrite\").parquet(spark_path_ratings)\n",
    "# books_clean.write.mode(\"overwrite\").parquet(spark_path_books)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1534c116",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
